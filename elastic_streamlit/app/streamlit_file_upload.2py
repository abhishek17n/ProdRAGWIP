import os
import requests
import json
import streamlit as st
from elasticsearch import Elasticsearch
import openai

from dotenv import load_dotenv

# Load environment variables from the .env file
load_dotenv()

# Embedding Service, Elasticsearch URLs, and OpenAI API Key
#EMBEDDING_SERVICE_URL = "http://172.17.0.4:8001/embed"
#ELASTICSEARCH_URL = "http://172.17.0.1:9200/documents/_doc" 
EMBEDDING_SERVICE_URL = "http://embedding_service:8001/embed"
ELASTICSEARCH_URL = "http://elasticdb:9200/documents/_doc" 
 # Replace 'documents' with your index name
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Initialize Elasticsearch client
es = Elasticsearch(hosts=["http://elasticdb:9200"])  # Change to your Elasticsearch URL if needed

# Define the index settings and mappings
index_name = 'documents'
settings = {
    "number_of_shards": 1,
    "number_of_replicas": 1
}

# Adding new fields: document_id, chunk_id, filename, chunk, and embedding
mappings = {
    "properties": {
        "document_id": {
            "type": "keyword"  # keyword for exact match
        },
        "chunk_id": {
            "type": "keyword"  # keyword for exact match
        },
        "filename": {
            "type": "text"  # text for full-text search capability
        },
        "chunk": {
            "type": "text"  # text for the chunk of the document
        },
        "embedding": {
            "type": "dense_vector",  # use dense vector for storing embeddings
            "dims": 768              # assuming embeddings have 768 dimensions
        }
    }
}

# Function to create an index
def create_index(index_name, settings, mappings):
    if not es.indices.exists(index=index_name):
        response = es.indices.create(
            index=index_name,
            body={
                "settings": settings,
                "mappings": mappings
            }
        )
        print(f"Index '{index_name}' created: {response}")
    else:
        print(f"Index '{index_name}' already exists")

# Function to generate embedding for a document using embedding service
def get_embedding_from_service(document):
    """Send a document to the embedding service to generate an embedding."""
    try:
        # Truncate the document if it exceeds the model's maximum sequence length
        max_length = 512
        truncated_document = document[:max_length]
        
        response = requests.post(EMBEDDING_SERVICE_URL, json={"query": truncated_document})
        
        if response.status_code == 200:
            # Return the embedding from the service response
            return response.json()["embedding"]
        else:
            raise Exception(f"Error in embedding service: {response.status_code}, {response.text}")
    
    except Exception as e:
        raise Exception(f"Failed to get embedding for document: {document}. Error: {str(e)}")

# Function to chunk text
import re

def chunk_text(text, chunk_size=500):
    """Split the document into chunks of a given size, trying not to break sentences."""
    current_chunk = []
    current_length = 0

    for sentence in re.split(r'(?<=[.!?]) +', text):
        if current_length + len(sentence) > chunk_size:
            yield ' '.join(current_chunk)
            current_chunk = []
            current_length = 0
        
        current_chunk.append(sentence)
        current_length += len(sentence)

    if current_chunk:
        yield ' '.join(current_chunk)

# Function to index a document along with its embedding and chunks

def index_document_with_embedding(document_id, filename, document_content):
    # Step 2: Chunk the document and index each chunk with its own embedding
    chunks = list(chunk_text(document_content))
    
    for chunk_num, chunk in enumerate(chunks):
        # Get embedding for each chunk
        embedding = get_embedding_from_service(chunk)
        
        # Ensure embedding is formatted correctly (list of floats)
        if not isinstance(embedding, list):
            raise ValueError(f"Invalid embedding format for chunk {chunk_num}")

        doc = {
            "document_id": document_id,
            "chunk_id": f"{document_id}_chunk_{chunk_num}",
            "filename": filename,
            "chunk": chunk,
            "embedding": embedding
        }

        # Use the appropriate argument based on your Elasticsearch Python client version
        response = es.index(index=index_name, id=f"{document_id}_chunk_{chunk_num}", document=doc)

        # Print a more concise response message
        print(f"Document with chunk ID '{document_id}_chunk_{chunk_num}' indexed: {response['result']}")

# Function to search for documents in Elasticsearch
def search_documents(query):
    """Search for documents in Elasticsearch based on the query."""
    try:
        # Generate embedding for the query using embedding service
        embedding = get_embedding_from_service(query)
        
        # Define the search query with a combination of cosine similarity and keyword matching
        search_body = {
            "query": {
                "bool": {
                    "must": [
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                                    "params": {"query_vector": embedding}
                                }
                            }
                        },
                        {
                            "match": {"chunk": query}
                        }
                    ]
                }
            }
        }
        
        # Perform the search
        response = es.search(index=index_name, body=search_body)
        hits = response['hits']['hits']
        
        if hits:
            return hits
        else:
            raise Exception("No results found in Elasticsearch")
    except Exception as e:
        st.error(f"An error occurred while searching: {str(e)}")
        return []



# Function to get a response from OpenAI if no results found in Elasticsearch
def get_response_from_openai(query, context=""):
    """Generate a response from OpenAI if no results are found in Elasticsearch."""
    try:
        openai.api_key = OPENAI_API_KEY
        prompt = f"Question: {query}\nContext: {context}\nProvide a helpful answer based on the given context."
        
        # Using ChatCompletion API with the newer model
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=150
        )
        
        return response.choices[0].message['content'].strip()
    
    except Exception as e:
        st.error(f"An error occurred while getting a response from OpenAI: {str(e)}")
        return ""


# Streamlit UI to upload file and process chunks
st.title("Document Chunk and Embedding Uploader/Searcher")

uploaded_file = st.file_uploader("Choose a text file")

if uploaded_file is not None:
    try:
        document_content = uploaded_file.read().decode("utf-8").strip()
    except UnicodeDecodeError:
        document_content = uploaded_file.read().decode("latin-1").strip()

    document_id = uploaded_file.name  # Using filename as document ID

    # Step 1: Generate Embeddings and Index the Document
    if st.button("Process and Upload"):
        try:
            st.write("Generating embeddings and indexing the document...")
            index_document_with_embedding(document_id, uploaded_file.name, document_content)
            st.success(f"File '{uploaded_file.name}' successfully processed and indexed.")
        except Exception as e:
            st.error(f"An error occurred: {str(e)}")

# Streamlit UI to search for documents
st.header("Search Documents")
query = st.text_input("Enter your search query")
if st.button("Search"):
    if query:
        results = search_documents(query)
        if results:
            st.write("Search Results:")
            for result in results:
                st.write(f"Document ID: {result['_source']['document_id']}")
                st.write(f"Filename: {result['_source']['filename']}")
                st.write(f"Chunk: {result['_source']['chunk']}")
                st.write("---")
        else:
            st.write("No results found in Elasticsearch. Generating response from OpenAI...")
            openai_response = get_response_from_openai(query)
            st.write(f"OpenAI Response: {openai_response}")
    else:
        st.error("Please enter a query to search.")

# Create the Elasticsearch index
create_index(index_name, settings, mappings)